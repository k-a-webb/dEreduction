
This guide was written by Kristi Webb, May 2015, as a follow up to a guide written by Gwen Rudie from 2005.


In this guide I will outline the reduction steps used to process Gemini GMOS IFU data of IC 225 taken in 2005 from Gemini-North by Bryan Miller. The observation details can be found in ‘GN-2005B-DD-5.xml’ and the raw data files are located in ‘/net/sbfmaps/Volumes/Science/bmiller/bdisk/bmiller/GN-2005B-DD-5/raw/‘.
I will also cover how the kinematic analysis was preformed both by Gwen with an IDL version of pPXF and how it has been updated to use techniques developed by James Turner in Python and then with the Python version of pPXF.

The work done by Gwen can be found at ‘/net/sbfmaps/Volumes/Science/bmiller/bdisk/bmiller/GN-2005B-DD-5/gwen’ and includes two guides (‘guide.txt’/‘gudie.txt’ and ‘idl_log.txt’) detailing her reduction steps.
I make an effort to reproduce her results but using the standard IRAF gmos tasks and following the reduction method developed by James Turner as used in his GMOS IFU example reduction script ‘allred.cl’.

    Contents:
	- Preprocessing and cubing
	    - preprocessing - Gwen’s method
	    - preprocessing - My method
	    - cubing and flattening - Gwen’s method
	    - cubing and flattening - my method

	- Analysis of differences

	- Kinematic analysis, Gwen’s method
	    - Binning
   	    - pPXF - first method
	    - pPXF - ‘easier to follow’ example
	    - plotting the results

	- Kinematic analysis with Python
	    - Flatten (optional, kinda, do here or after preprocessing)
	    - Scrop flatten (optional)
	    - Binning
   	    - pPXF
	    - plotting the results
	    - fxcor
	    - rvsoa
	    - check if velocity shift between CCDs

Michele Cappellari’s Voronoi binning and pPXF programs can be found here: http://www-astro.physics.ox.ac.uk/~mxc/software/



————————————————————————
Preprocessing and cubing
————————————————————————

  cl>  cl < reduce_and_cube.cl

PREPROCESSING
—————————————

GWENS METHOD —————

		INPUT: raw IFU data: /net/sbfmaps/Volumes/Science/bmiller/bdisk/bmiller/GN-2005B-DD-5/raw/
			various scripts of Bryan’s: iraf/scripts/gmos/
			various scripts of Gwen’s: iraf/scripts/gwen/
			chip gap files:iraf/scripts/gmos/data/chipgaps.dat
			bad pixel mask files: iraf/scripts/gmos/bpms/gn_bpm_ccd1_*.pl
			extinction file: iraf/scripts/gmos/calib/mkoextinct.dat
			* potentially missing other things?
		OUTPUT: output of preprocessing: cslqtexbrgN20051205S0006.fits, cslqtexbrgN20051222S0108.fits, 
						 cslqtexbrgN20051222S0112.fits, cslqtexbrgN20051223S0121.fits
			3D cube by gwen’s method: IC225_cal_decimal.fits, IC225_cal_integer.fits

Quick and dirty outline:
Gwen does her science reduction and cubing (as outlined in ‘guide.txt’) with three ‘cl’ scripts and a series of IRAF tasks developed by Bryan Miller (which differ slightly from the IRAF standard tasks). The scripts are: ’preliminary_reduction.cl’ which does the preprocessing, ‘gfcube_MEF_script.cl’ to reorganise the data into 3D cubes, and ‘shift_and_combine.cl’ to shift and combine the cubes by interger and decimal shifts. The IRAF tasks she uses can be found in her folder kept by Bryan (‘/net/sbfmaps/Volumes/Science/bmiller/bdisk/bmiller/GN-2005B-DD-5/gwen/reu2/iraf/scripts’) which includes Bryan’s original ‘ifuproc.cl’ which handles the majority of the reduction of the science images. She recommends cropping the final cubes which contain blank sections. She then uses her version of ‘scrop.cl’ to crop the cube  in the desired wavelength range, and flattens the cube into 2D with imexamine with ‘project=yes’.

More details can be found in her guides (described above), I will highlight the differences between her reduction and the updated version as I go along.

Gwen’s scripts rewritten (but do the same):
I have reformated Gwen’s script ‘preliminary_reduction.cl’ for visual clarity in the script ‘prelim_reduc_torun.cl’ if you prefer to read over this version. It has the exact smae functionality. Gwen’s scripts ‘gfcube_MEF_script.cl’ and ‘shift_and_combine.cl’ were rewritten as the tasks ‘make_cube.cl’ and ‘align_cubes.cl’ respectively, and are called at the end of the reduction script. They have the same functionality as Gwen’s versions.

She notes in her guide that all steps are automated except for the velocity shift correction and the selection of points for the response function. 

    To apply the velocity shift correction (as written in Gwen’s guide):
	- Run the science, flat, twilight, and arc images through ifuproc
	- Calculate the velocity shift correction from the arcs (furthur details in the script)
	- Shift the science spectra in the opposite way as the arcs

    To interactively calculate the response funciton ‘sens*.fits’:
	- run gsstandard interactively
	- ‘d’ to delete points on spectral lines
	- ‘aa’ to select opposite corners of a box around new points
	- ‘q’ save and quit


MY METHOD —————

		INPUT:  raw IFU data: /net/sbfmaps/Volumes/Science/bmiller/bdisk/bmiller/GN-2005B-DD-5/raw/
			updated ifuproc: iraf/scripts/gmos/ifuproc_gqecorr.cl
			James’ IRAF scripts: iraf/extern/ifudrgmos/gmos/
			James’ Python scripts: iraf/extern/pyfu-0.8.1/
			Gwen’s gscombine: iraf/scripts/gwen/gscombine.cl
			chip gap files: iraf/scripts/gmos/data/chipgaps.dat
			bad pixel mask files: iraf/scripts/gmos/bpms/gn_bpm_ccd1_*.pl
			extinction file: iraf/scripts/gmos/calib/mkoextinct.dat
			* potentially missing other things?
		OUTPUT: output of preprocessing: csteqpxbprgN20051205S0006.fits, csteqpxbprgN20051222S0108.fits, 
						 csteqpxbprgN20051222S0112.fits, csteqpxbprgN20051223S0121.fits
			3D cube by james’ method: dcsteqpxbprgN20051205S0006_add.fits

The reduction method I use is similar to that of James, with the inclusion of an updated version of ‘ifuproc.cl’ (instead of repeating the same steps for the standard and sciece, the ifuproc task is used). The updated ‘ifuproc’ is ‘ifuproc_gqecorr.cl’ where the steps have been rearranged to allow for the input to meet the necessary parameters to run the IRAF standard ‘gqecorr.cl’ rather than Bryan’s version of the task.

The preproccessing script (and for the cubing) is named ‘reduce_and_cube.cl’.

	- I’m not sure why Gwen specifies the wavelength parameters ‘dw=.91 nw=1301 w1=4186’ but I preserve these in my own code. 
          This results in a wavelength range of [4186:5369.91]

As we wanted to be able to preform the analysis with the standard IRAF tasks (rather than the tasks developed by Bryan) for the purpose of reproducability, it was necessary to rearrange the reduction steps in ‘ifuproc.cl’ as the ’gqecorr.cl’ input cannot be mosaiced. Parameters, such as ‘gsigma’ in Bryan’s gfextract, which were not included in the standard tasks were removed. Extensive testing was preformed to ensure that the these parameters were not necessary and could be removed without significantly altering the final product.

This updated version of ifuproc was named ‘ifuproc_gqecorr.cl’. 

	- The parameter ‘gsigma’ in gfextract (and thus gfreduce) is a smoothing term for the noise in the region
	  where the CCDs overlap. This results in a decrease in the noise on the right of the spectrum. This is 
	  not included in the standard IRAF task, and we believe that it is best to not include it in the updated
	  reduction, as the noise is not significant to our analysis. (?)

	- The quantum efficiency correction could not be implemented by ‘gfreduce’ as it does in Bryan’s version
	  (although the parameter fl_qecorr is written into the current task, it is not yet working). Instead
	  I call ‘gqecorr’ explicitly before ‘gfreduce’ as done by James.

	- Gwen uses a version of ‘gscombine’ to combine the standards used to produce the wavelength calibration
	  function. It is a version of the iraf standard scombine with slight changes to allow for a selection 
 	  based on wavelength rather than pixels. This version is not the same as Bryan’s version, the difference
	  culminating in a different vertical (magnitude) shift. To properly reproduce Gwen’s results, her version
	  of the task was used.

	- The wavelength calibration function (sens_{whatever}.fits) as output by gsstandard was produced by 
	  running the task interactively and ensuring that the data points were not on spectral lines. Although
	  the function I produced was very similar to that of Gwen’s, it was not the exact same shape. 
	**** Test to see if this has a significant effect

In order to determine the best reduction method, a comparison was made to find the minimized wavelength shift between the two IFU slits. The various methods tested were: Gwen’s original method as detailed in her script ‘preliminary_reduction.cl’, a reproduction of Gwen’s method using her version of scripts and IRAF tasks, using James’ reduction method with Bryan’s IRAF tasks (which require the original ifuproc), James’ method with the standard IRAF tasks, and James’ method with James’ IRAF tasks (which include an improved method of flat- fielding). The comparison between the methods is shown in ‘wavelength shift.txt’. Also see the section “Check if velocity shift between CCDs” for more information.

The best method was James’ reduction method with James’ IRAF tasks, so this was used to preform the reduction. As we expect James’ methods in his tasks will be included in the next update of the gmos tasks we do not worry about whether others cannot reproduce our results. The only task of James’ that I did not use was ‘gfresponse.cl’ as it is currently not capable of handling twilights.

	- The main differences in the reduction is the order of the steps (as can be seen by the differing file
	  prefixes) as well as preforming the overscan subtraction right away, calculating the VAR and DQ plane
	  and carrying them through the reduction stages, writing the BPM from the GMOS IFU provided BPM files 
	  rather than specifying specific regions, and using the updated versions of the IRAF tasks. I also use 
	  gemfix (with addbpm) rather than nmisc as it seemed to provide better results.


CUBING AND FLATTENING
—————————————————————

		
GWENS METHOD —————

		INPUT:  gwen’s integer combined 3D cube: IC225_cal_integer_crop.fits
		OUTPUT: gwen’s 2D images: 2D_IC225_cal_integer_crop_sci.fits, 2D_IC225_cal_integer_crop_var.fits

Gwen has written the cl scripts ‘gfcube_MEF_scrip.cl’ and ‘shift_and_combine.cl’ to create the 3D cubes. Note here the input coordinate list for ‘shift_and_combine.cl’ is ‘mkim_coords.dat’. It is necessary to have this file in the same folder as the preprocessing files.
Alternatively I have written these scripts as iraf tasks, ‘make_cube.cl’ and ‘align_cubes.cl’.

The output of Gwen’s method of making the 3D cubes is: ‘IC225_cal_integer.fits’ and ‘IC225_cal_decimal.fits’. Because her method results in blank sections, she then crops the files: ‘IC225_cal_integer_crop.fits’, ‘IC225_cal_decimal_crop.fits’. I have done my best to reproduce her ‘cropping’ with the image dimensions below, while making sure to remove columns/rows with 0 signal or noise. Gwen’s dimensions were 72x53, mine are 68x49.

And then she uses ‘imcombine.cl’ with ‘project=yes’ to flatten the science and variance extensions: ‘2D_IC225_cal_integer_crop_sci.fits’, ‘2D_IC225_cal_integer_crop_var.fits’, etc.

  cl>  tcopy IC225_cal_integer.fits[MDF] IC225_cal_integer_crop.fits[MDF]
  cl>  imcopy IC225_cal_integer.fits[SCI][1:68,12:60,0:1300] IC225_cal_integer_crop.fits[SCI]
  cl>  imcopy IC225_cal_integer.fits[VAR][1:68,12:60,0:1300] IC225_cal_integer_crop.fits[VAR]
  cl>  imcopy IC225_cal_integer.fits[DQ][1:68,12:60,0:1300] IC225_cal_integer_crop.fits[DQ]

- make sure to use Gwen’s version of scrop (not Bryan’s), must be in gmos folder in IRAF for gimverify to work
  cl>  scrop IC225_cal_integer_crop.fits IC225_cal_integer_crop_scrop.fits section="4800:5366" fl_vardq+

  cl>  imcombine IC225_cal_integer_crop.fits[sci] 2D_IC225_cal_integer_crop_sci.fits project=yes
  cl>  imcombine IC225_cal_integer_crop.fits[var] 2D_IC225_cal_integer_crop_var.fits project=yes



MY METHOD —————

		INPUT:  james’ 3D cube: dcsteqpxbprgN20051205S0006_add.fits
		OUTPUT: james’ 2D images: IC225_2D_james_sci.fits, IC225_2D_james_var.fits

James has written an IRAF package ‘pyfu’ which includes the tasks ‘pyfalign’, ‘pyfmosaic’, and ‘pyflogbin’ to align, mosaic etc. the 3D data cubes output from ‘gfcube.cl’. I use these along with Gwens of ‘make_cube’ and ‘align_cubes’ to determine the difference. The mosaic’d output is ‘dcsteqpxbprgN20051205S0006_add.fits’ and ‘IC225_cal_integer_crop.fits’ (named the same as Gwen’s).

	- James’ mosaicing method does not require manual cropping. 

	- I don’t use James’ ‘pyflogbin’ as the log binning is handled in pPXF.

	- In recreating Gwen’s result I kept encountering some unidentfied source of a sigma clipping (obvious 
	  when you visually compare Gwen’s results with mine) during the mosaicing process. I cannot explain 
	  what this is a result of. It should not matter in the long run as I will later determine it doesn’t
	  make any sense to use James’ reduction method with Gwen’s cubing method.

I then flatten the science and variance extension of the 3D cube to be processed by the voronoi 2D binning method.

This can also be acomplised in the script described below (‘do_all.py’) if you’d rather avoid IRAF at all costs… (good luck with that).
  
  cl>  imcombine dcsteqpxbprgN20051205S0006_add.fits[sci] IC225_2D_sci.fits project=yes
  cl>  imcombine dcsteqpxbprgN20051205S0006_add.fits[var] IC225_2D_var.fits project=yes



———————————————————————
Analysis of differences
———————————————————————


As the results of the two methods were more different with respect to S/N than was expected we did a careful inspection of how the variance plane is calculated througout the reduction. This may be more information than you ever need to know. 

TLDR: Gwen’s result is wrong (by no fault of her own). The result of James’ reduction and cubing looks correct.

The mean S/N of the raw data is ~2.8, therefore the end S/N should be within 20% of this (the error being a result of the inability of all of the flux to be extracted by the algorithm in ‘gfextract’). Gwen’s cube ‘IC225_cal_integer_crop.fits’ has a mean S/N of 5.13 (and my reproduction of her result has S/N of 5.05 so this this doesnt look like an anamolous error). James’ cube has a S/N of 2.76. Using James’ method of reduction together with Gwen’s method of cubing gives a S/N of 79.8, so something has gone horribly wrong. I don’t bother trying to figure out what though, as James’ result seems really good.

Gwen’s error seems to be a result of the old method that ‘gfextract’ calculated the variance plane in its flux extraction. It seems that the error reported by ‘apall’ is not correct so James’ has worked to simply propagate the variance plane through this step rather than work with the ‘apall’ error measurement. This is an issue in Gwen’s method as she first calculated her variance plane in the extraction step. The mean of her initial variance plane is the exact same as her science plane - so it’s obvious that there was an error. As the new standard process is to calculate the data quality and variance planes right off the bat (in the first bias subtraction with gfreduce) and carry the planes through each step (fl_vardq+) we can see that Gwen’s inital variance differs by about a factor of 5 from where it should be (which is the same factor the science plane gets multiplied by from xbprgN20051205S0006.fits to exbprgN20051205S0006.fits). This factor gets carried through the rest of the process until the noise is calculated from the variance. The additional factor of 5 in the variance will result in (sqrt(5) ~ 2.5) difference in the noise. This accounts for the difference between my result and Gwen’s.



—————————————————————————————————————————————
Kinematic analysis - Gwen’s method (with IDL)
—————————————————————————————————————————————

	- I choose to use my own naming convention for the file structure rather than follow Gwen’s, although they are pretty similar. (Sorry for any confusion)

BINNING
———————

The first step is to make the input file for the voronoi binning process with ‘make_table.pro’ which records the signal and noise measurements for each pixel. An important difference here is that ‘gfcube’ now (since 2014) records the flux in ‘counts/arcsec^s’ rather than just ‘counts’. But this will not effect the calculation of S/N so we can use the same calculation as Gwen.

  IDL>  .r idl/cappellari/make_table.pro
  IDL>  make_table

		INPUT: 2D_IC225_cal_integer_crop_sci.fits, 2D_IC225_cal_integer_crop_var.fits
		OUTPUT: idl_proc/x_y_signal_noise.txt

        - In Gwen’s IDL guide she says to use ‘idl/cappellari/voronoi_2d_binning/pre_bin_fits_readin.pro’ but
          I could not get this to work, so I used the similar, but perhaps less fancy, ‘make_table.pro’
          ******** which has the file names and dimensions hardcoded in. ********
	  I followed the same naming conventions as Gwen specified in her IDL guide.

	- In some versions fo ‘make_table.pro’ in Gwen’s directory the variance measurements are not square-rooted
	  (we assume here the noise is the square-root of the variance plane alone). Make sure that the version you
	  use calculates the noise properly; not speaking from experiance……

Now run the binning program ‘v2b_IC225_centroid.pro’ which requires ‘voronoi_2d_binning.pro’. 
  
    You will need to change:
	- input filename
	- the parameter NUMLINE (number of lines in the input file + 1)
	- desired signal to noise. (I follow Gwen’s example and use S/N of 30 as my test case.)

    - Load some files into IDL (not sure how to get it to do this automatically)
    - I’m not sure what version of IDL Gwen had, but I had to manually load ‘strsplit.pro’ (used in ‘rdfloat.pro’) from
	the ‘old’ exelis IDL folder I  was given, as well as some others… 

  IDL>  .r idl/cappellari/voronoi_2d_binning/voronoi_2d_binning.pro
  IDL>  .r /Users/kwebb/idl/astron_2005dec21/pro/misc/numlines.pro
  IDL>  .r /Applications/exelis/idl83/lib/strsplit.pro
  IDL>  .r /Users/kwebb/idl/astrolib.20110516/pro/misc/rdfloat.pro
  IDL>  .r /Applications/exelis/idl83/lib/mean.pro

  IDL>  .r idl/cappellari/voronoi_2d_binning/v2b_IC225_centroid.pro
  IDL>  v2b_IC225_centroid

	**** To be able to run this you need to remove the rows with 0.0000 noise from the input file

****** WHY?, SEEMS SHADY ************************************************

		INPUT: idl_proc/x_y_signal_noise.txt
		OUTPUT: idl_proc/v2b_output_sn30.txt, idl_proc/v2b_output_sn30_XBIN_cent.txt, 
			idl_proc/v2b_output_sn30_YBIN_cent.txt, idl_proc/v2b_output_sn30_XBIN_gen.txt,
			idl_proc/v2b_output_sn30_YBIN_gen.txt

Now to make the binned spectra. Gwen uses a script ‘make_binned_spectra.cl’ to do the commands but it’s a mess of cl and awk. So I’ll just write it out here instead (using my file structure). Gwen has 127 bins at S/N 30. (Remember that her SN was calculated wrong so we should not expect to get the same result as her here).

  $  chmod 755 idl_proc/PHP_write_awk.php
  $  ./gwen_idl/idl_proc/PHP_write_awk.php
  cl>  cl < gwen_idl/idl_proc/awk_program.cl
  cl>  for (i=0; i<=127; i+=1) {
  >>>>  imcombine(“@idl_proc/comlis30/imcom_"//i//".lis",("idl_proc/bin_spec_s_n_30/bin_000"+i)//".fits",combine="sum")
  >>>>  }

And repeat for the variance plane. Same procedure, with ‘PHP_write_awk_var.php’, ‘awk_program_var.php’ and in folder ‘gwen_idl/idl_proc/imcom_var/‘. This makes a series of folders with the lists of spectra in the same bin, and then the imcombine’d fits images.

		INPUT: v_2d_b_IC225_out_30.txt, IC225_cal_integer_crop.fits
		OUTPUT: lists of pixels in the same bin: idl_proc/imcom_lis/imcom_*.lis, idl_proc/imcom_var/imcom_*.lis
			lists of pixels and their respective bin: idl_proc/comlis30/imcom_*.lis
			imcombined fits images of same bin: bin_spec_s_n_30/bin_*.fits



pPXF 
————

There is a variety of ways that Gwen preforms this step, I recommend reading the whole section before doing anything to save youself a headache…

I’m not completely sure I understand what Gwen does here because in her longer, first, example she includes the cropping step, but the script ‘ppxf_IC225_1.pro’ she uses uses the uncropped fits image. But later in this same example she calls ‘ppxf_IC225_crop.pro’ which DOES uses the cropped binned fits images. It does not say where she runs ‘ppxf_IC225_1.pro’. I suspect this may have been a mistake in her guide? In either of these scripts the velocity/sigma information which is used in the plotting is not saved to a file, whereas it is in the script in her ‘easier to follow’ example. Instead Gwen uses a few lines of awk to extract this information later on.

I WOULD RECOMMEND USING THE SCRIPT AND METHOD OF THE ‘EASIER TO FOLLOW’ EXAMPLE for this reason.

In each of her scripts Gwen uses a different template spectra. It’s not clear what conclusion she came to as to which template was the best nor for what reason. In this step it should not really matter which template it used however.

    - ‘ppxf_IC225_1.pro’ uses template bc2003*
	- I think gwen uses ‘ppxf_IC225_1.pro’ to develop a template which does not require the galaxy spectra to be cropped. 
    - ‘ppxf_IC225_crop.pro’ uses template Rbi*
    - ‘ppxf_IC225_full_length_30_s_n.pro’ uses template Mun*

FIRST METHOD ———————

The main file that runs pPXF is ‘idl/cappellari/ppxf/ppxf_IC225_1.pro’ and requires the hardcoded parameters be changed:
    -  data specific header keywords (eg. CD3_3 instead of CDELT3)
    -  if a different Lamrange calculation is desired, comment out the LAMRNGE calcultion and replace with your own
    -  the template spectra and its information (eg. sigma and instrumental resolution)
	-  modify the parameter sigma = ____/velScale if necessary based on calculations (?)
    -  the GOODPIXEL ranges (pixel ranges that DO NOT contain emission lines)
	-  can use splot ‘$.’ (?) option where the ‘$’ changes the image to pixel coordinates
	-  Gwen has a nice reminder here: “THESE GOODPIXEL RANGES MUST BE FROM THE LOGARITHMICALLY REBINNED DATA”
    -  the galaxy spectrum
    -  the start parameter (inital guess) for velocity and sigma ~ [1500., 100.]

Crop the spectra to the wavelenth range available in the template spectra:

	**** I believe this is done with the IRAF standard scrop, not gwen’s version (?)

  cl> scrop idl_proc/comb_fits_sci_30/bin_sci_100.fits idl_proc/comb_fits_sci_30/bin_sci_crop_100.fits w1=4800 w2=5366

    - I have written a script, ‘crop_spectra.cl’, which does this for each file.
		INPUT: idl_proc/comb_fits_sci_30/bin_sci_*.fits
		OUTPUT: idl_proc/comb_fits_sci_30/bin_sci_crop_*.fits

Determine the GOODPIXELS on the Log rebinned data. Gwen does this in IDL with ‘log_rebin_1.pro’. The python equivalent of ‘log_rebin.pro’ is in the Python distribution of pPXF in the script ‘ppxf_util.py’. To follow Gwen’ we rebin from w1=4900 to w2=5300. I have written this in python: ‘IFU_reduction/log_rebin_IC225.py’.
An example input and output: ‘idl_proc/comb_fits_sci_30/bin_sci_crop_100.fits’, ‘idl_proc/comb_fits_sci_30/bin_sci_crop_log_100.fits’. These are hardcoded at the moment. 

  IDL>  .r /Users/kwebb/idl/cappellari/ppxf/log_rebin_1D.pro

Then Gwen says to use the script ‘ppxf_IC225_crop.pro’. She is not explicit as to where she actually runs ‘ppxf_IC225_1.pro’ but I’m guessing it is after she runs ‘ppxf_IC225_crop.pro’. But then the input of ‘ppxf_IC225_crop.pro’ is the cropped version of the files, so maybe ‘ppxf_IC225_1.pro’ is actually run first??? To obtain the velocity information used to make the plots Gwen uses a script ‘ppxf_IC225_crop.pro’ which seems like the same as ‘ppxf_IC225_full_length_30_s_n.pro’ except that it uses a different template spectra, Vazdekis - which are the Rbi1.30z* files - and the spectral range [4800, 5366] (rather than [4186,5369] in ‘ppxf_IC225_full_length_30_s_n.pro’). The input of this script is the cropped version of the binned fits files. 

Load some other files from an old IDL version, such as loadct.pro (perhaps because of my IDL setup this is necessary for me)

  IDL>  .r /Applications/exelis/idl83/lib/loadct.pro

  then run:

  IDL>  .r /Users/kwebb/idl/cappellari/ppxf/ppxf_IC225_crop.pro


Run after ‘ppxf_IC225_crop.pro’ (?) I’m not even sure if it’s necessary as it has the same functionality but with a different template spectra

  IDL>  .r /Users/kwebb/idl/cappellari/ppxf/ppxf_IC225_1.pro

Gwen includes a step here using fxcor on the binned UNcropped fits images for all the bins she has (543)

  cl>  for (i=0; i<=543; i+=1) {
  >>>>    fxcor( (“idl_proc/bin_spec_000/bin_000”+i)//“.fits”, “idl_proc/bin_spec_000/bin_101.fits”, 
  >>>>            output=(“idl_proc/bin_spec_000/fxcor/bin_fxcor_000”+i)//“.fits”, continuum=“both”,
  >>>>            osample=“4500-5100”, rsample=“4500-5100”, interactive- )

   Then in the folder ‘idl_proc/bin_spec_000/fxcor/‘ she extracts the velocity information:

  $  !find . -name ‘*.txt’ | sort | xargs tail -q -n 1 | awk ‘{print $2,$23}’ > vrel_name_binned.lis
  $  !find . -name ‘*.txt’ | sort | xargs tail -q -n 1 | awk ‘{print $2,$23}’ > vrel_per_bin.lis



EASIER TO FOLLOW EXAMPLE ———————	

    -  In order to use one of the other template spectra’s, the galaxy spectra must be cropped to the same wavelength
       range as the template. (?)

The main file that runs this is ‘ppxf_IC225_full_length_30_s_n.pro’, the same parameters as above need to be changed depending on its use. In this script she uses the template spectra Mun instead of bc2003* which is implemented more similarly to the example script provided in the pPXF download.

The script incorperates the logarithmic rebinning so it does not need to be done explicitly (?). However the GOODPIXELS do need to be determined. Gwen selects: [range(2,195),range(235,790),range(830,890),range(930,940),range(980,1300)]
  
Then we can run the ppxf script, 

  IDL>  .r /Users/kwebb/idl/cappellari/ppxf/ppxf_IC225_full_length_30_s_n.pro
  IDL>  ppxf_IC225_full_length_30_s_n

		INPUT: idl_proc/bin_spec_s_n_30/bin_*fits, idl/cappellari/ppxf/spectra/Mun1.30z*.fits
		OUTPUT: idl_proc/ppxf/vel_sigma_out_s_n_30.txt, idl_proc/ppxf/vel_out_s_n_30.txt,
			idl_proc/ppxf/ppfx_bestfit_s_n_30.fits, idl_proc/ppxf/ppxf_error_s_n_30.fits



PLOTTING THE RESULTS
————————————————————

Make the overplot data and the continuum spectral region with a series of tasks in IRAF:

  cl>  for (i=0;i<=127;i+=1) {
  >>>>    imcombine("@idl_proc/comlis30/imcom_"//i//".lis", ("idl_proc/bin_spec_s_n_30/flux_000"+i)//".fits", combine="average")
  >>>>    scopy(("idl_proc/bin_spec_s_n_30/flux_000"+i)//".fits",("idl_proc/bin_spec_s_n_30/cont_flux_000"+i)//".fits",w1=4370,w2=4870)
  >>>>    }
  cl>  imstat idl_proc/bin_spec_s_n_30/cont_flux_*.fits fields="mean" format = no > idl_proc/bin_spec_s_n_30/binned_cont_flux.lis

		INPUT: idl_proc/comlis30/imcom_*.lis
		OUTPUT: idl_proc/bin_spec_s_n_30/binned_cont_flux.lis

Then the results are plotted with the script ‘/idl/cappellari/misc/plot_ppxf_30.pro’

  IDL>  .r /Users/kwebb/idl/cappellari/misc/plot_ppxf_30.pro
  IDL>  plot_ppxf_30

		INPUT: idl_proc/v2b_output_sn30_xbin_cent, idl_proc/v2b_output_sn30_ybin_cent, idl_proc/ppxf/vel_out_s_n_30.txt, 
			idl_proc/bin_spec_s_n_30/binned_cont_flux.lis
		OUTPUT: 




——————————————————————————————————
Kinematic analysis - Python method
——————————————————————————————————

To do absolutely everything!!! I kid, but everything is in here.

	$ python do_all.py

All the steps necessary to create the input spectra for pPXF are handled in the script ‘do_all.py’ as well as running pPXF. I will outline the steps in the script below, there are also brief descriptions in the script itself.

	- instead of using the folder name ‘idl_proc’ I use ‘doall_proc_{S/N}’ because it makes more sense dammit.

By comparison of several different S/N_min it looks like a S/N of ~20 gives good enough resolution for the kinematic analysis.

The S/N, file names, and path names are hardcoded in at the top of the ‘do_all.py’ script, so they will need to be edited manually according to how you want to sue the script. Also the methods you use are in the bottom __init__ method, uncomment them as needed. They should work according to the variabels named at the top of the script.

FLATTEN (optional step depending on how you did the last of the preprocessing)
———————

This just takes the cube and flattens it (for the whole spectral range!) into 2D. It runs imcombine the same way as I described above and as Gwen did, jsut conveniently implemented in Python!

		INPUT: dcsteqpxbprgN20051205S0006_add.fits
		OUTPUT: IC225_2D_sci.fits, IC225_2D_var.fits

SCROP FLATTEN (again, optional, basically just for visual comparison of different wavelength ranges)
—————————————

This just involves cropping the 3D cube to a specific spectral range before flattening. The wavelength ranges I examine include:
    - OIII: 4980-5035 A (4959-5700/5368.54488701)
    - H_beta: 4883-4887 A
    - H_gamma: 4360-4362 A
    - continuum: 4370-4870 A

		INPUT: dcsteqpxbprgN20051205S0006_add.fits
		OUTPUT: IC225_2D_sci_{lambda_start}_{lambda_end}.fits, IC225_2D_var_{lambda_start}_{lambda_end}.fits


BINNING - the ACTUAL start of the analysis
———————

The first step is to create the input file for the voronoi 2D binning which consists of a text file with the x/y coordinates and the corresponding signal to noise. I used the same method as gwen in ‘make_table.pro’. 

As the noise must be positive values in ‘voronoi_2d_binning.py’ make sure to remove any lines where the variance == 0. 

**** MAYBE NEED TO CROP IMAGE??? MAYBE WRITE IN SOMETHING TO AUTOMATICALLY IGNORE THESE VALUES??? ********************
**** SHOULD WE IGNORE THE NEGATIVE VALUES OR TABLE ABSOLUTE VALUES???? ********************

		INPUT: IC225_2D_sci.fits, IC225_2D_var.fits
		OUTPUT: doall_proc_20/x_y_signal_noise.txt

Then run the voronoi binning which outputs a list of x/y coordinates and their respective bin number in a text file ‘doall_proc_20/v2b_output_sn30.txt’ and the other coordinate measurements in ‘doall_proc_20/v2b_output_sn30_xy.txt’. And example of the console output is shown at the bottom of the do_all script.

		INPUT: doall_proc_20/x_y_signal_noise.txt
		OUTPUT: doall_proc_20/v2b_output_sn20.txt, doall_proc_20/v2b_output_sn20_xy.txt, 

Before this can be put through pPXF the spectra need to be organized with respect to their bin. This involves making lists of which spectra are in each bin, then combining the spectra of the same bin. Gwen does this with her script ‘make_binned_spectra.cl’ which uses awk to produce a file ‘awkprogram.cl’ which you then run in IRAf. I do something equivalent in Python.
Basically she uses imcombine to combine all the spectra of the same bin in ‘bin’ fits files.
Despite choose a smaller minimum bin value than Gwen (this is because the S/N I calculate differs from Gwen’s, see Analysis of differences section) the S/N I have requires many spectra to be combined to meat this minimum.
I cannot use imcombine because it routinely fails when there are too many images. So I have have rewritten an equivalent method in Python. 
Also in this step I produce flux fits files, where the average (rather than the sum which is used to increase the S/N) flux of the combined images can be calculated. 


		INPUT: dcsteqpxbprgN20051205S0006_add.fits, doall_proc_20/v2b_output_sn20.txt
		OUTPUT: doall_proc_20/comb_fits_sci_20/bin_sci_*.fits, doall_proc_20/comb_fits_sci_20/flux_sci_*.fits
			doall_proc_20/comb_fits_var_20/bin_var_*.fits, doall_proc_20/comb_fits_var_20/flux_var_*.fits

The combined spectra (flux summed) will be what you input into pPXF.


pPXF
————

I have rewritten an example script in the latest version of the Python pPXF distribution to use the same template spectra as Gwen - BC2003. It has the same functionality as ‘idl/cappellari/ppxf/ppxf_IC225_1.pro’, and can be found in ‘ppxf_IC225_kinematics.py’. I do not use this template in ‘do_all.py’ however.

I think Gwen’s script (ppxf_IC225_1.pro) may be an outdated script. In her ‘easier to follow’ section of her IDL guide she uses ‘ppxf_IC225_full_length_30_s_n.pro’ instead. This script uses a different template - Mun, with it being implemented more like the Sauron example script which is provided with the pPXF download. I have rewritten this as ‘ppxf_IC225_fulllength.py’. This has *almost* the same functionaliy, with the exception of the second for loop which logarithmically rebinns all the galaxy spectra as this step was not included in the Sauron example. This is the template I use in ‘do_all.py’, and I follow the same method here as in ‘ppxf_IC225_fulllength.py’

The names of the template files and the 2D image file are hardcoded and will require the file to be edited, as well as with the changes specified above for Gwen’s equivalent script. Except the goodpixels. This is now handled by a python function included inthe new ppxf util, and it seems to work  very well.

I have also adapted the way that gwen calculates the ‘sigma’ (I’m a bit confused how she does this actually) to work with the actual resolution of the synthetic spectra Munori template (http://adsabs.harvard.edu/abs/2005A%26A...442.1127). I use (See the script for more details). It’s one of the templates that is recommened on Cappelari’s website. 

The logarithmic rebinning of the cropped spectra is handled in either ppxf* script rather than having to do it explicitly. I had rewritten gwen’s script in python though, ‘log_rebin_IC225.py’ before I realized the step wasn’t necessary; so it’s there if you need it……

Before we run pPXF though we need to determine a good penalty (BIAS) value to use for our data. See Cappellari’s readme.txt in her pPXF distribution for more details (actually it’s rather sparse on details).


So now we can run pPXF. An example of the output can be found at the end of the script.



————— MONTE CARLO for errors

First run pPXF with BIAS=0. Observe the h3 h4 terms along with their respective errors in the output text file. The values of both which are close the limit of -0.3 and +0.3 typically are null measurements. We therefore consider only the values with small errors and those not around the limit (ie with values in the range -0.15-0.15 only and with errors less than 1) -> h3=-0.003146 h4=0.000112583

Now put these values in the python monte carlo simulation method. According to Cappellari’s instruction for this you only need to change the h3 and h4 variables(?) and not ‘sigmaV’ or ‘vel’ (?). It didn’t seen to make a big difference anyways so I left them as they were in teh template.

I select a bias of 0.6. Put this into the actual run of pPXF

————— run pPXF

Make sure you have the proper bias value.

		INPUT: doall_proc_20/comb_fits_sci_20/bin_sci_*.fits, idl/cappellari/ppxf/spectra/Mun1.30z*.fits
		OUTPUT:  doall_proc_20/ppxf_output_sn20.txt, doall_proc_20/ppxf_proc/bestfit_*.fits


PLOTTING THE RESULTS
————————————————————

First calculate the average flux in each bin for a given spectral range (i.e. the full range available). This is done in ‘scopy_flux’.

		INPUT: doall_proc_20/comb_fits_sci_20/flux_sci_*.fits, spectral range
		OUTPUT: doall_proc_20/binned_flux_20.txt

Then read in the velocity information from the pPXF output, and the spatial information from the voronoi binning output. Throw all this into ‘plot_velfield_setup’ which setup’s Cappellari’s method ‘plot_velfield’. Observe output plot. Ooo pretty colours.

		INPUT: doall_proc_20/binned_flux_20.txt, doall_proc_20/ppxf_output_sn20.txt, doall_proc_20/v2b_output_sn20.txt


FXCOR
—————

So we want to able to compare different methods to do the kinematic analysis. pPXF is great because it automatically rejects emission lines in order to calculate the stellar velocity field from the absorption spectrum alone. Great! But let’s see what we can do with fxcor, which is perhaps a more traditional program to use. 

First issue - how do we get the absorption spectra to do the stellar kinematic analysis? (i.e. delete those pesky emission lines)
Well, pPXF applied a pretty fancy technique to optimize template spectra selection, and outputs the best fitting template into a fits file (while I wrote in the feature to save it as a fits file). This should represent the absorption spectrum pretty well, and we can use it to remove the emission lines. Great! This involves subtracting the bestfitting template fromthe original spectra leaving the emission lines. We cannot subtract the emission lines from the original spectra directly though (that would leave us with the best fitting template), so you would need to convolved the emission lines with a gaussian, then subtract them. This is the technique initially used by the SAURON project (http://adsabs.harvard.edu/abs/2001MNRAS.326...23B). In their third paper though (http://adsabs.harvard.edu/abs/2004MNRAS.352..721E) Cappellari’s technique has been around for a while and they use this instead, stating the results are equivalent to their earlier technique. Great! So we should expect both to give the same result. Do we? ******************************************


What we can do easily with fxcor is the kinematic analysis of the emission lines. Select a spaxel bin with a nice high S/N, preferably in the center of the galaxy, to use as the template, and run! In the output plot (input the velocity information from fxcor into plot_velfield_setup jsut like we did for pPXF) we can see that there’s an obvious velocity difference top to bottom across the galaxy indicative or rotation. Or is it? 

The next hing would be to ensure that this is not an artificial effect of the difference in applying the same wavelength solution in the preprocessing to both the CCDs despite slight differences. Spoiler, it doesn’t look like an artificial effect. See section ‘Check if velocity shift between CCDs’ below for more information about this.

		INPUT: doall_proc_20/comb_fits_sci_20/bin_sci_*.fits
		OUTPUT: doall_proc_20/fxcor_proc/bin_sci_sn20_list.lis, doall_proc_20/fxcor_proc/fxcor_bin_sci_sn20_tmpl6.txt,
			doall_proc_20/fxcor_proc/em_bin_sci_sn20_list.lis, doall_proc_20/fxcor_proc/fxcor_em_bin_sci_sn20_tmpl6.txt


RVSAO
—————

So we can do a kinematic analysis of the stellar kinematics with pPXF and with fxcor (albeit complicatedly, and should get the same results). But surely there is an IRAF task which does the same cross correllation as fxcor but removes the emission features?! And there is! Well, in theory.

The rvsao task xcsao should be able to preform the same cross correlation (see help for details about this) as fxcor while chopping off the emission lines. The trick here is getting it to work. *************************************

Well what about the velocity velocity field of the emission lines? The task emsao can calculate this. And we expect the results from this task to closely match that of the fxcor emission line kinematics. 

Again we check if the apparently rotation is an effect of the difference between the two CCDs. Again it looks like it’s not.


CHECK IF VELOCITY SHIFT BETWEEN CCDS
————————————————————————————————————

In the plots we want to ensure the ‘rotation’ feature we can see (i.e. the velocity difference at the top of the galaxy and the bottom) is not a feature of a shift in the wavelength correction between the CCDs. So let’s go back to the preprocessing and calculated the velocity/pixel shift needed to correct for the difference in the correction. I do this with the script ‘wavelength_shift_check/reduce_and_cube_with_shift.cl’ which basically just includes the task ‘gfshift’ to apply the shift. In order to calculate the shift needed for each galaxy image, refer to the ‘Measure velocity difference between…’ section in the script. It’s the example Gwen has in her script actually. I’ll highlight the steps. 

Transform the first arc, make sure to use Bryan’s version of gftransform and in IRAF (pyraf gives a wird error with one of the variables)
   cl > gftransform ergN20051205S0008.fits wavtraname=ergN20051205S0008
Determine the correction, use Gwen’s version of gfxcor in pyraf (iraf gives a weird error, convenient eh?). Continue in pyraf.
   ——> gfxcor tergN20051205S0008.fits obs=Gemini-North
Separate out the regions of the two CCDs
   ——> tselect tergN20051205S0008.fits[mdf] slit1.fits "NO <= 750"
   ——> tselect tergN20051205S0008.fits[mdf] slit2.fits "NO >= 751"
Calculate the difference, first in pixels then in relative velocity
   ——> tstat s008_1.fits,s008_2.fits SHIFT lowlim=-9999.0 highlim=15.
   ——> tstat s008_1.fits,s008_2.fits VREL lowlim=-9999.0 highlim=10.
The difference is the mean of s008_2.fits minus s008_1.fits (Then the shift is the negative of this, just to be arbitrarily confusing with the math, but also to follow Gwen’s example)
Repeat for the second arc, the shift correction is then the average of the two values. 
The velocity shift is just informative, I do nothing with it. The shift applied is in pixels.

I use this method also to determine which of the reduction methods is the best - i.e. the best method requires the smallest shift. 


—— Notes

in doall_sci_20:
	- cleaned noise in bin 54

in doall_sci_20_shift:
	- cleaned noise in bin 30


