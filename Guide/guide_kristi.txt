
This guide was written by Kristi Webb, May 2015, as a follow up to a guide written by Gwen Rudie from 2005.


In this guide I will outline the reduction steps used to process Gemini GMOS IFU data of IC 225 taken in 2005 from Gemini-North by Bryan Miller. The observation details can be found in ‘GN-2005B-DD-5.xml’ and the raw data files are located in ‘/net/sbfmaps/Volumes/Science/bmiller/bdisk/bmiller/GN-2005B-DD-5/raw/‘.
I will also cover how the kinematic analysis was preformed both by Gwen with an IDL version of pPXF and how it has been updated to use techniques developed by James Turner in Python and then with the Python version of pPXF.

The work done by Gwen can be found at ‘/net/sbfmaps/Volumes/Science/bmiller/bdisk/bmiller/GN-2005B-DD-5/gwen’ and includes two guides (‘guide.txt’/‘gudie.txt’ and ‘idl_log.txt’) detailing her reduction steps.
I make an effort to reproduce her results but using the standard IRAF gmos tasks and following the reduction method developed by James Turner as used in his GMOS IFU example reduction script ‘allred.cl’.

    Contents:
	- Preprocessing and cubing
	    - preprocessing - Gwen’s method
	    - preprocessing - My method
	    - cubing and flattening - Gwen’s method
	    - cubing and flattening - my method

	- Analysis of differences

	- Kinematic analysis, Gwen’s method
	    - Binning
   	    - pPXF - first method
	    - pPXF - ‘easier to follow’ example
	    - plotting the results

	- Kinematic analysis with Python
	    - Binning
   	    - pPXF
	    - plotting the results



————————————————————————
Preprocessing and cubing
————————————————————————

  cl>  cl < reduce_and_cube.cl

PREPROCESSING
—————————————

GWENS METHOD —————

		INPUT: raw IFU data: /net/sbfmaps/Volumes/Science/bmiller/bdisk/bmiller/GN-2005B-DD-5/raw/
			various scripts of Bryan’s: iraf/scripts/gmos/
			various scripts of Gwen’s: iraf/scripts/gwen/
			chip gap files:iraf/scripts/gmos/data/chipgaps.dat
			bad pixel mask files: iraf/scripts/gmos/bpms/gn_bpm_ccd1_*.pl
			extinction file: iraf/scripts/gmos/calib/mkoextinct.dat
			* potentially missing other things?
		OUTPUT: output of preprocessing: cslqtexbrgN20051205S0006.fits, cslqtexbrgN20051222S0108.fits, 
						 cslqtexbrgN20051222S0112.fits, cslqtexbrgN20051223S0121.fits
			3D cube by gwen’s method: IC225_cal_decimal.fits, IC225_cal_integer.fits

Quick and dirty outline:
Gwen does her science reduction and cubing (as outlined in ‘guide.txt’) with three ‘cl’ scripts and a series of IRAF tasks developed by Bryan Miller (which differ slightly from the IRAF standard tasks). The scripts are: ’preliminary_reduction.cl’ which does the preprocessing, ‘gfcube_MEF_script.cl’ to reorganise the data into 3D cubes, and ‘shift_and_combine.cl’ to shift and combine the cubes by interger and decimal shifts. The IRAF tasks she uses can be found in her folder kept by Bryan (‘/net/sbfmaps/Volumes/Science/bmiller/bdisk/bmiller/GN-2005B-DD-5/gwen/reu2/iraf/scripts’) which includes Bryan’s original ‘ifuproc.cl’ which handles the majority of the reduction of the science images. She recommends cropping the final cubes which contain blank sections. She then uses her version of ‘scrop.cl’ to crop the cube  in the desired wavelength range, and flattens the cube into 2D with imexamine with ‘project=yes’.

More details can be found in her guides (described above), I will highlight the differences between her reduction and the updated version as I go along.

Gwen’s scripts rewritten (but do the same):
I have reformated Gwen’s script ‘preliminary_reduction.cl’ for visual clarity in the script ‘prelim_reduc_torun.cl’ if you prefer to read over this version. It has the exact smae functionality. Gwen’s scripts ‘gfcube_MEF_script.cl’ and ‘shift_and_combine.cl’ were rewritten as the tasks ‘make_cube.cl’ and ‘align_cubes.cl’ respectively, and are called at the end of the reduction script. They have the same functionality as Gwen’s versions.

She notes in her guide that all steps are automated except for the velocity shift correction and the selection of points for the response function. 

    To apply the velocity shift correction (as written in Gwen’s guide):
	- Run the science, flat, twilight, and arc images through ifuproc
	- Calculate the velocity shift correction from the arcs (furthur details in the script)
	- Shift the science spectra in the opposite way as the arcs

    To interactively calculate the response funciton ‘sens*.fits’:
	- run gsstandard interactively
	- ‘d’ to delete points on spectral lines
	- ‘aa’ to select opposite corners of a box around new points
	- ‘q’ save and quit


MY METHOD —————

		INPUT:  raw IFU data: /net/sbfmaps/Volumes/Science/bmiller/bdisk/bmiller/GN-2005B-DD-5/raw/
			updated ifuproc: iraf/scripts/gmos/ifuproc_gqecorr.cl
			James’ IRAF scripts: iraf/extern/ifudrgmos/gmos/
			James’ Python scripts: iraf/extern/pyfu-0.8.1/
			Gwen’s gscombine: iraf/scripts/gwen/gscombine.cl
			chip gap files: iraf/scripts/gmos/data/chipgaps.dat
			bad pixel mask files: iraf/scripts/gmos/bpms/gn_bpm_ccd1_*.pl
			extinction file: iraf/scripts/gmos/calib/mkoextinct.dat
			* potentially missing other things?
		OUTPUT: output of preprocessing: csteqpxbprgN20051205S0006.fits, csteqpxbprgN20051222S0108.fits, 
						 csteqpxbprgN20051222S0112.fits, csteqpxbprgN20051223S0121.fits
			3D cube by james’ method: dcsteqpxbprgN20051205S0006_add.fits

The reduction method I use is similar to that of James, with the inclusion of an updated version of ‘ifuproc.cl’ (instead of repeating the same steps for the standard and sciece, the ifuproc task is used). The updated ‘ifuproc’ is ‘ifuproc_gqecorr.cl’ where the steps have been rearranged to allow for the input to meet the necessary parameters to run the IRAF standard ‘gqecorr.cl’ rather than Bryan’s version of the task.

The preproccessing script (and for the cubing) is named ‘reduce_and_cube.cl’.

	- I’m not sure why Gwen specifies the wavelength parameters ‘dw=.91 nw=1301 w1=4186’ but I preserve these in my own code. 
          This results in a wavelength range of [4186:5369.91]

As we wanted to be able to preform the analysis with the standard IRAF tasks (rather than the tasks developed by Bryan) for the purpose of reproducability, it was necessary to rearrange the reduction steps in ‘ifuproc.cl’ as the ’gqecorr.cl’ input cannot be mosaiced. Parameters, such as ‘gsigma’ in Bryan’s gfextract, which were not included in the standard tasks were removed. Extensive testing was preformed to ensure that the these parameters were not necessary and could be removed without significantly altering the final product.

This updated version of ifuproc was named ‘ifuproc_gqecorr.cl’. 

	- The parameter ‘gsigma’ in gfextract (and thus gfreduce) is a smoothing term for the noise in the region
	  where the CCDs overlap. This results in a decrease in the noise on the right of the spectrum. This is 
	  not included in the standard IRAF task, and we believe that it is best to not include it in the updated
	  reduction, as the noise is not significant to our analysis. (?)

	- The quantum efficiency correction could not be implemented by ‘gfreduce’ as it does in Bryan’s version
	  (although the parameter fl_qecorr is written into the current task, it is not yet working). Instead
	  I call ‘gqecorr’ explicitly before ‘gfreduce’ as done by James.

	- Gwen uses a version of ‘gscombine’ to combine the standards used to produce the wavelength calibration
	  function. It is a version of the iraf standard scombine with slight changes to allow for a selection 
 	  based on wavelength rather than pixels. This version is not the same as Bryan’s version, the difference
	  culminating in a different vertical (magnitude) shift. To properly reproduce Gwen’s results, her version
	  of the task was used.

	- The wavelength calibration function (sens_{whatever}.fits) as output by gsstandard was produced by 
	  running the task interactively and ensuring that the data points were not on spectral lines. Although
	  the function I produced was very similar to that of Gwen’s, it was not the exact same shape. 
	**** Test to see if this has a significant effect

In order to determine the best reduction method, a comparison was made to find the minimized wavelength shift between the two IFU slits. The various methods tested were: Gwen’s original method as detailed in her script ‘preliminary_reduction.cl’, a reproduction of Gwen’s method using her version of scripts and IRAF tasks, using James’ reduction method with Bryan’s IRAF tasks (which require the original ifuproc), James’ method with the standard IRAF tasks, and James’ method with James’ IRAF tasks (which include an improved method of flat- fielding). The comparison between the methods is shown in ‘wavelength shift.txt’.

The best method was James’ reduction method with James’ IRAF tasks, so this was used to preform the reduction. As we expect James’ methods in his tasks will be included in the next update of the gmos tasks we do not worry about whether others cannot reproduce our results. The only task of James’ that I did not use was ‘gfresponse.cl’ as it is currently not capable of handling twilights.

	- The main differences in the reduction is the order of the steps (as can be seen by the differing file
	  prefixes) as well as preforming the overscan subtraction right away, calculating the VAR and DQ plane
	  and carrying them through the reduction stages, writing the BPM from the GMOS IFU provided BPM files 
	  rather than specifying specific regions, and using the updated versions of the IRAF tasks. I also use 
	  gemfix (with addbpm) rather than nmisc as it seemed to provide better results.


CUBING AND FLATTENING
—————————————————————

		
GWENS METHOD —————

		INPUT:  gwen’s integer combined 3D cube: IC225_cal_integer_crop.fits
		OUTPUT: gwen’s 2D images: 2D_IC225_cal_integer_crop_sci.fits, 2D_IC225_cal_integer_crop_var.fits

Gwen has written the cl scripts ‘gfcube_MEF_scrip.cl’ and ‘shift_and_combine.cl’ to create the 3D cubes. Note here the input coordinate list for ‘shift_and_combine.cl’ is ‘mkim_coords.dat’. It is necessary to have this file in the same folder as the preprocessing files.
Alternatively I have written these scripts as iraf tasks, ‘make_cube.cl’ and ‘align_cubes.cl’.

The output of Gwen’s method of making the 3D cubes is: ‘IC225_cal_integer.fits’ and ‘IC225_cal_decimal.fits’. Because her method results in blank sections, she then crops the files: ‘IC225_cal_integer_crop.fits’, ‘IC225_cal_decimal_crop.fits’. I have done my best to reproduce her ‘cropping’ with the image dimensions below, while making sure to remove columns/rows with 0 signal or noise. Gwen’s dimensions were 72x53, mine are 68x49.

And then she uses ‘imcombine.cl’ with ‘project=yes’ to flatten the science and variance extensions: ‘2D_IC225_cal_integer_crop_sci.fits’, ‘2D_IC225_cal_integer_crop_var.fits’, etc.

  cl>  tcopy IC225_cal_integer.fits[MDF] IC225_cal_integer_crop.fits[MDF]
  cl>  imcopy IC225_cal_integer.fits[SCI][1:68,12:60,0:1300] IC225_cal_integer_crop.fits[SCI]
  cl>  imcopy IC225_cal_integer.fits[VAR][1:68,12:60,0:1300] IC225_cal_integer_crop.fits[VAR]
  cl>  imcopy IC225_cal_integer.fits[DQ][1:68,12:60,0:1300] IC225_cal_integer_crop.fits[DQ]

- make sure to use Gwen’s version of scrop (not Bryan’s), must be in gmos folder in IRAF for gimverify to work
  cl>  scrop IC225_cal_integer_crop.fits IC225_cal_integer_crop_scrop.fits section="4800:5366" fl_vardq+

  cl>  imcombine IC225_cal_integer_crop.fits[sci] 2D_IC225_cal_integer_crop_sci.fits project=yes
  cl>  imcombine IC225_cal_integer_crop.fits[var] 2D_IC225_cal_integer_crop_var.fits project=yes



MY METHOD —————

		INPUT:  james’ 3D cube: dcsteqpxbprgN20051205S0006_add.fits
		OUTPUT: james’ 2D images: IC225_2D_james_sci.fits, IC225_2D_james_var.fits

James has written an IRAF package ‘pyfu’ which includes the tasks ‘pyfalign’, ‘pyfmosaic’, and ‘pyflogbin’ to align, mosaic etc. the 3D data cubes output from ‘gfcube.cl’. I use these along with Gwens of ‘make_cube’ and ‘align_cubes’ to determine the difference. The mosaic’d output is ‘dcsteqpxbprgN20051205S0006_add.fits’ and ‘IC225_cal_integer_crop.fits’ (named the same as Gwen’s).

	- James’ mosaicing method does not require manual cropping. 

	- I don’t use James’ ‘pyflogbin’ as the log binning is handled in pPXF.

	- In recreating Gwen’s result I kept encountering some unidentfied source of a sigma clipping (obvious 
	  when you visually compare Gwen’s results with mine) during the mosaicing process. I cannot explain 
	  what this is a result of. It should not matter in the long run as I will later determine it doesn’t
	  make any sense to use James’ reduction method with Gwen’s cubing method.

I then flatten the science and variance extension of the 3D cube to be processed by the voronoi 2D binning method.
  
  cl>  imcombine dcsteqpxbprgN20051205S0006_add.fits[sci] IC225_2D_sci.fits project=yes
  cl>  imcombine dcsteqpxbprgN20051205S0006_add.fits[var] IC225_2D_var.fits project=yes



———————————————————————
Analysis of differences
———————————————————————


As the results of the two methods were more different with respect to S/N than was expected we did a careful inspection of how the variance plane is calculated througout the reduction. This may be more information than you ever need to know. 

TLDR: Gwen’s result is wrong (by no fault of her own). The result of James’ reduction and cubing looks correct.

The mean S/N of the raw data is ~2.8, therefore the end S/N should be within 20% of this (the error being a result of the inability of all of the flux to be extracted by the algorithm in ‘gfextract’). Gwen’s cube ‘IC225_cal_integer_crop.fits’ has a mean S/N of 5.13 (and my reproduction of her result has S/N of 5.05 so this this doesnt look like an anamolous error). James’ cube has a S/N of 2.76. Using James’ method of reduction together with Gwen’s method of cubing gives a S/N of 79.8, so something has gone horribly wrong. I don’t bother trying to figure out what though, as James’ result seems really good.

Gwen’s error seems to be a result of the old method that ‘gfextract’ calculated the variance plane in its flux extraction. It seems that the error reported by ‘apall’ is not correct so James’ has worked to simply propagate the variance plane through this step rather than work with the ‘apall’ error measurement. This is an issue in Gwen’s method as she first calculated her variance plane in the extraction step. The mean of her initial variance plane is the exact same as her science plane - so it’s obvious that there was an error. As the new standard process is to calculate the data quality and variance planes right off the bat (in the first bias subtraction with gfreduce) and carry the planes through each step (fl_vardq+) we can see that Gwen’s inital variance differs by about a factor of 5 from where it should be (which is the same factor the science plane gets multiplied by from xbprgN20051205S0006.fits to exbprgN20051205S0006.fits). This factor gets carried through the rest of the process until the noise is calculated from the variance. The additional factor of 5 in the variance will result in (sqrt(5) ~ 2.5) difference in the noise. This accounts for the difference between my result and Gwen’s.



—————————————————————————————————————————————
Kinematic analysis - Gwen’s method (with IDL)
—————————————————————————————————————————————

	- I choose to use my own naming convention for the file structure rather than follow Gwen’s, although they are pretty similar. (Sorry for any confusion)

BINNING
———————

The first step is to make the input file for the voronoi binning process with ‘make_table.pro’ which records the signal and noise measurements for each pixel. An important difference here is that ‘gfcube’ now (since 2014) records the flux in ‘counts/arcsec^s’ rather than just ‘counts’. But this will not effect the calculation of S/N so we can use the same calculation as Gwen.

  IDL>  .r idl/cappellari/make_table.pro
  IDL>  make_table

		INPUT: 2D_IC225_cal_integer_crop_sci.fits, 2D_IC225_cal_integer_crop_var.fits
		OUTPUT: idl_proc/x_y_signal_noise.txt

        - In Gwen’s IDL guide she says to use ‘idl/cappellari/voronoi_2d_binning/pre_bin_fits_readin.pro’ but
          I could not get this to work, so I used the similar, but perhaps less fancy, ‘make_table.pro’
          ******** which has the file names and dimensions hardcoded in. ********
	  I followed the same naming conventions as Gwen specified in her IDL guide.

	- In some versions fo ‘make_table.pro’ in Gwen’s directory the variance measurements are not square-rooted
	  (we assume here the noise is the square-root of the variance plane alone). Make sure that the version you
	  use calculates the noise properly; not speaking from experiance……

Now run the binning program ‘v2b_IC225_centroid.pro’ which requires ‘voronoi_2d_binning.pro’. 
  
    You will need to change:
	- input filename
	- the parameter NUMLINE (number of lines in the input file + 1)
	- desired signal to noise. (I follow Gwen’s example and use S/N of 30 as my test case.)

    - Load some files into IDL (not sure how to get it to do this automatically)
    - I’m not sure what version of IDL Gwen had, but I had to manually load ‘strsplit.pro’ (used in ‘rdfloat.pro’) from
	the ‘old’ exelis IDL folder I  was given, as well as some others… 

  IDL>  .r idl/cappellari/voronoi_2d_binning/voronoi_2d_binning.pro
  IDL>  .r /Users/kwebb/idl/astron_2005dec21/pro/misc/numlines.pro
  IDL>  .r /Applications/exelis/idl83/lib/strsplit.pro
  IDL>  .r /Users/kwebb/idl/astrolib.20110516/pro/misc/rdfloat.pro
  IDL>  .r /Applications/exelis/idl83/lib/mean.pro

  IDL>  .r idl/cappellari/voronoi_2d_binning/v2b_IC225_centroid.pro
  IDL>  v2b_IC225_centroid

	**** To be able to run this you need to remove the rows with 0.0000 noise from the input file

****** WHY, SEEMS SHADY ************************************************

		INPUT: idl_proc/x_y_signal_noise.txt
		OUTPUT: idl_proc/v2b_output_sn30.txt, idl_proc/v2b_output_sn30_XBIN_cent.txt, 
			idl_proc/v2b_output_sn30_YBIN_cent.txt, idl_proc/v2b_output_sn30_XBIN_gen.txt,
			idl_proc/v2b_output_sn30_YBIN_gen.txt

Now to make the binned spectra. Gwen uses a script ‘make_binned_spectra.cl’ to do the commands but it’s a mess of cl and awk. So I’ll just write it out here instead (using my file structure). Gwen has 127 bins at S/N 30. (Remember that her SN was calculated wrong so we should not expect to get the same result as her here).

  $  chmod 755 idl_proc/PHP_write_awk.php
  $  ./gwen_idl/idl_proc/PHP_write_awk.php
  cl>  cl < gwen_idl/idl_proc/awk_program.cl
  cl>  for (i=0; i<=127; i+=1) {
  >>>>  imcombine(“@idl_proc/comlis30/imcom_"//i//".lis",("idl_proc/bin_spec_s_n_30/bin_000"+i)//".fits",combine="sum")
  >>>>  }

And repeat for the variance plane. Same procedure, with ‘PHP_write_awk_var.php’, ‘awk_program_var.php’ and in folder ‘gwen_idl/idl_proc/imcom_var/‘. This makes a series of folders with the lists of spectra in the same bin, and then the imcombine’d fits images.

		INPUT: v_2d_b_IC225_out_30.txt, IC225_cal_integer_crop.fits
		OUTPUT: lists of pixels in the same bin: idl_proc/imcom_lis/imcom_*.lis, idl_proc/imcom_var/imcom_*.lis
			lists of pixels and their respective bin: idl_proc/comlis30/imcom_*.lis
			imcombined fits images of same bin: bin_spec_s_n_30/bin_*.fits



pPXF 
————

There is a variety of ways that Gwen preforms this step, I recommend reading the whole section before doing anything to save youself a headache…

I’m not completely sure I understand what Gwen does here because in her longer, first, example she includes the cropping step, but the script ‘ppxf_IC225_1.pro’ she uses uses the uncropped fits image. But later in this same example she calls ‘ppxf_IC225_crop.pro’ which DOES uses the cropped binned fits images. It does not say where she runs ‘ppxf_IC225_1.pro’. I suspect this may have been a mistake in her guide? In either of these scripts the velocity/sigma information which is used in the plotting is not saved to a file, whereas it is in the script in her ‘easier to follow’ example. Instead Gwen uses a few lines of awk to extract this information later on.

I WOULD RECOMMEND USING THE SCRIPT AND METHOD OF THE ‘EASIER TO FOLLOW’ EXAMPLE for this reason.

In each of her scripts Gwen uses a different template spectra. It’s not clear what conclusion she came to as to which template was the best nor for what reason. In this step it should not really matter which template it used however.

    - ‘ppxf_IC225_1.pro’ uses template bc2003*
	- I think gwen uses ‘ppxf_IC225_1.pro’ to develop a template which does not require the galaxy spectra to be cropped. 
    - ‘ppxf_IC225_crop.pro’ uses template Rbi*
    - ‘ppxf_IC225_full_length_30_s_n.pro’ uses template Mun*

FIRST METHOD ———————

The main file that runs pPXF is ‘idl/cappellari/ppxf/ppxf_IC225_1.pro’ and requires the hardcoded parameters be changed:
    -  data specific header keywords (eg. CD3_3 instead of CDELT3)
    -  if a different Lamrange calculation is desired, comment out the LAMRNGE calcultion and replace with your own
    -  the template spectra and its information (eg. sigma and instrumental resolution)
	-  modify the parameter sigma = ____/velScale if necessary based on calculations (?)
    -  the GOODPIXEL ranges (pixel ranges that DO NOT contain emission lines)
	-  can use splot ‘$.’ (?) option where the ‘$’ changes the image to pixel coordinates
	-  Gwen has a nice reminder here: “THESE GOODPIXEL RANGES MUST BE FROM THE LOGARITHMICALLY REBINNED DATA”
    -  the galaxy spectrum
    -  the start parameter (inital guess) for velocity and sigma ~ [1500., 100.]

Crop the spectra to the wavelenth range available in the template spectra:

	**** I believe this is done with the IRAF standard scrop, not gwen’s version (?)

  cl> scrop idl_proc/comb_fits_sci_30/bin_sci_100.fits idl_proc/comb_fits_sci_30/bin_sci_crop_100.fits w1=4800 w2=5366

    - I have written a script, ‘crop_spectra.cl’, which does this for each file.
		INPUT: idl_proc/comb_fits_sci_30/bin_sci_*.fits
		OUTPUT: idl_proc/comb_fits_sci_30/bin_sci_crop_*.fits

Determine the GOODPIXELS on the Log rebinned data. Gwen does this in IDL with ‘log_rebin_1.pro’. The python equivalent of ‘log_rebin.pro’ is in the Python distribution of pPXF in the script ‘ppxf_util.py’. To follow Gwen’ we rebin from w1=4900 to w2=5300. I have written this in python: ‘IFU_reduction/log_rebin_IC225.py’.
An example input and output: ‘idl_proc/comb_fits_sci_30/bin_sci_crop_100.fits’, ‘idl_proc/comb_fits_sci_30/bin_sci_crop_log_100.fits’. These are hardcoded at the moment. 

  IDL>  .r /Users/kwebb/idl/cappellari/ppxf/log_rebin_1D.pro

Then Gwen says to use the script ‘ppxf_IC225_crop.pro’. She is not explicit as to where she actually runs ‘ppxf_IC225_1.pro’ but I’m guessing it is after she runs ‘ppxf_IC225_crop.pro’. But then the input of ‘ppxf_IC225_crop.pro’ is the cropped version of the files, so maybe ‘ppxf_IC225_1.pro’ is actually run first??? To obtain the velocity information used to make the plots Gwen uses a script ‘ppxf_IC225_crop.pro’ which seems like the same as ‘ppxf_IC225_full_length_30_s_n.pro’ except that it uses a different template spectra, Vazdekis - which are the Rbi1.30z* files - and the spectral range [4800, 5366] (rather than [4186,5369] in ‘ppxf_IC225_full_length_30_s_n.pro’). The input of this script is the cropped version of the binned fits files. 

Load some other files from an old IDL version, such as loadct.pro (perhaps because of my IDL setup this is necessary for me)

  IDL>  .r /Applications/exelis/idl83/lib/loadct.pro

  then run:

  IDL>  .r /Users/kwebb/idl/cappellari/ppxf/ppxf_IC225_crop.pro


Run after ‘ppxf_IC225_crop.pro’ (?) I’m not even sure if it’s necessary as it has the same functionality but with a different template spectra

  IDL>  .r /Users/kwebb/idl/cappellari/ppxf/ppxf_IC225_1.pro

Gwen includes a step here using fxcor on the binned UNcropped fits images for all the bins she has (543)

  cl>  for (i=0; i<=543; i+=1) {
  >>>>    fxcor( (“idl_proc/bin_spec_000/bin_000”+i)//“.fits”, “idl_proc/bin_spec_000/bin_101.fits”, 
  >>>>            output=(“idl_proc/bin_spec_000/fxcor/bin_fxcor_000”+i)//“.fits”, continuum=“both”,
  >>>>            osample=“4500-5100”, rsample=“4500-5100”, interactive- )

   Then in the folder ‘idl_proc/bin_spec_000/fxcor/‘ she extracts the velocity information:

  $  !find . -name ‘*.txt’ | sort | xargs tail -q -n 1 | awk ‘{print $2,$23}’ > vrel_name_binned.lis
  $  !find . -name ‘*.txt’ | sort | xargs tail -q -n 1 | awk ‘{print $2,$23}’ > vrel_per_bin.lis



EASIER TO FOLLOW EXAMPLE ———————	

    -  In order to use one of the other template spectra’s, the galaxy spectra must be cropped to the same wavelength
       range as the template. (?)

The main file that runs this is ‘ppxf_IC225_full_length_30_s_n.pro’, the same parameters as above need to be changed depending on its use. In this script she uses the template spectra Mun instead of bc2003* which is implemented more similarly to the example script provided in the pPXF download.

The script incorperates the logarithmic rebinning so it does not need to be done explicitly (?). However the GOODPIXELS do need to be determined. Gwen selects: [range(2,195),range(235,790),range(830,890),range(930,940),range(980,1300)]
  
Then we can run the ppxf script, 

  IDL>  .r /Users/kwebb/idl/cappellari/ppxf/ppxf_IC225_full_length_30_s_n.pro
  IDL>  ppxf_IC225_full_length_30_s_n

		INPUT: idl_proc/bin_spec_s_n_30/bin_*fits, idl/cappellari/ppxf/spectra/Mun1.30z*.fits
		OUTPUT: idl_proc/ppxf/vel_sigma_out_s_n_30.txt, idl_proc/ppxf/vel_out_s_n_30.txt,
			idl_proc/ppxf/ppfx_bestfit_s_n_30.fits, idl_proc/ppxf/ppxf_error_s_n_30.fits



PLOTTING THE RESULTS
————————————————————

Make the overplot data and the continuum spectral region with a series of tasks in IRAF:

  cl>  for (i=0;i<=127;i+=1) {
  >>>>    imcombine("@idl_proc/comlis30/imcom_"//i//".lis", ("idl_proc/bin_spec_s_n_30/flux_000"+i)//".fits", combine="average")
  >>>>    scopy(("idl_proc/bin_spec_s_n_30/flux_000"+i)//".fits",("idl_proc/bin_spec_s_n_30/cont_flux_000"+i)//".fits",w1=4370,w2=4870)
  >>>>    }
  cl>  imstat idl_proc/bin_spec_s_n_30/cont_flux_*.fits fields="mean" format = no > idl_proc/bin_spec_s_n_30/binned_cont_flux.lis

		INPUT: idl_proc/comlis30/imcom_*.lis
		OUTPUT: idl_proc/bin_spec_s_n_30/binned_cont_flux.lis

Then the results are plotted with the script ‘/idl/cappellari/misc/plot_ppxf_30.pro’

  IDL>  .r /Users/kwebb/idl/cappellari/misc/plot_ppxf_30.pro
  IDL>  plot_ppxf_30

		INPUT: idl_proc/v2b_output_sn30_xbin_cent, idl_proc/v2b_output_sn30_ybin_cent, idl_proc/ppxf/vel_out_s_n_30.txt, 
			idl_proc/bin_spec_s_n_30/binned_cont_flux.lis
		OUTPUT: 




——————————————————————————————————
Kinematic analysis - Python method
——————————————————————————————————

	$ python do_all.py

All the steps necessary to create the input spectra for pPXF are handled in the script ‘do_all.py’ as well as running pPXF. I will outline the steps in the script below, there are also brief descriptions in the script itself.

	- instead of using the folder name ‘idl_proc’ I use ‘doall_proc_{S/N}’ because it makes more sense dammit.

By comparison of several different S/N_min it looks like a S/N of ~20 gives good enough resolution for the kinematic analysis

BINNING
———————

The S/N, file names, and path names are hardcoded in at the top of the ‘do_all.py’ script, so they will need to be edited manually.

The first step is to create the input file for the voronoi 2D binning which consists of a text file with the x/y coordinates and the corresponding signal to noise. I used the same method as gwen in ‘make_table.pro’. 

As the noise must be positive values in ‘voronoi_2d_binning.py’ make sure to remove any lines where the varanice == 0. 

**** MAYBE NEED TO CROP IMAGE??? MAYBE WRITE IN SOMETHING TO AUTOMATICALLY IGNORE THESE VALUES??? ********************
**** SHOULD WE IGNORE THE NEGATIVE VALUES OR TABLE ABSOLUTE VALUES???? ********************

		INPUT: IC225_2D_james_sci.fits, IC225_2D_james_var.fits
		OUTPUT: doall_proc_20/x_y_signal_noise.txt

Then run the voronoi binning which outputs a list of x/y coordinates and their respective bin number in a text file ‘ppxf_proc/v2b_output_sn30.txt’ and the other coordinate measurements in ‘ppxf_proc/v2b_output_sn30_xy.txt’. And example of the console output is shown at the bottom of the script.

		INPUT: doall_proc_20/x_y_signal_noise.txt
		OUTPUT: doall_proc_20/v2b_output_sn20.txt, doall_proc_20/v2b_output_sn20_xy.txt, 

Before this can be put through pPXF the spectra need to be organized with respect to their bin. This involves making lists of which spectra are in each bin, then combining the spectra of the same bin. Gwen does this with her script ‘make_binned_spectra.cl’ which uses awk to produce a file ‘awkprogram.cl’ which you then run in IRAf. I do something equivalent in Python.
Despite choose a smaller minimum bin value than Gwen (this is because the S/N I calculate differs from Gwen’s, see Analysis of differences section) it 


		INPUT: dcsteqpxbprgN20051205S0006_add.fits, doall_proc_20/v2b_output_sn20.txt
		OUTPUT: doall_proc_20/comb_lists_sci_20/imcomb_sci_*.lis, doall_proc_{S/N}/comb_lists_var_30/imcomb_var_*.lis,
			doall_proc_20/comb_fits_sci_20/bin_sci_*.fits, doall_proc_{S/N}/comb_fits_var_30/bin_var_*.fits

The combined spectra (in my case ppxf_proc/comb_fits_sci_30/bin_sci_*.fits) will be what you input into pPXF


pPXF
————

I have rewritten an example script in the latest version of the Python pPXF distribution to use the same template spectra as Gwen - BC2003. It has the same functionality as ‘idl/cappellari/ppxf/ppxf_IC225_1.pro’, and can be found in ‘ppxf_IC225_kinematics.py’. I do not use this template in ‘do_all.py’ however.

I think Gwen’s script (ppxf_IC225_1.pro) may be an outdated script. In her ‘easier to follow’ section of her IDL guide she uses ‘ppxf_IC225_full_length_30_s_n.pro’ instead. This script uses a different template - Mun, with it being implemented more like the Sauron example script which is provided with the pPXF download. I have rewritten this as ‘ppxf_IC225_fulllength.py’. This has *almost* the same functionaliy, with the exception of the second for loop which logarithmically rebinns all the galaxy spectra as this step was not included in the Sauron example. This is the template I use in ‘do_all.py’, and I follow the same method here as in ‘ppxf_IC225_fulllength.py’

The names of the template files and the 2D image file are hardcoded and will require the file to be edited, as well as with the changes specified above for Gwen’s equivalent script.

The logarithmic rebinning of the cropped spectra is handled in either ppxf* script rather than having to do it explicitly. I had rewritten gwen’s script in python though, ‘log_rebin_IC225.py’ before I realized the step wasn’t necessary; so it’s there if you need it……

So now we can run ‘ppxf_IC225_fulllength.py’. An example of the output can be found at the end of the script.

		INPUT: ppxf_proc/comb_fits_sci_30/bin_sci_*.fits, idl/cappellari/ppxf/spectra/Mun1.30z*.fits
		OUTPUT:  ppxf_proc/ppxf/vel_sigma_output_sn30.txt, ppxf_proc/ppxf/vel_output_sn30.txt, 
			ppxf_proc/ppxf/ppxf_bestfit_sn30.fits, ppxf_proc/ppxf/ppxf_error_sn30.fits

MONTE CARLO

When running with bias=0 observe the h3 h4 terms along with their respective errors. The values of both which are close the limit of -0.3 and +0.3 typically are null measurements. We therefore consider only the values with small errors and those not around the limit (ie with values in the range -0.15-0.15 only and with errors less than 1) -> h3=-0.003146 h4=0.000112583

In selecting the velocity we estimated the systematic velocity and subtracted this from the measured velocity in pPXF = 120 km/s
V / velscale = 120 / 51 = 2.35



PLOTTING THE RESULTS
————————————————————

Basically the same as Gwen’s method as I haven’t figured out how to use scrop in Python


Make the overplot data and the continuum spectral region, the same as Gwen, just conveniently written in the script 
  ‘idl_proc/make_overplot_data.cl’

  cl>  cl < make_overplot_data.cl

		INPUT: @comb_lists_sci_30/imcomb_sci_*.lis
		OUTPUT: bin_spec_sn30/flux_*.fits, bin_spec_sn30/cont_flux_*.fits, bin_spec_sn30/binned_cont_flux.lis

	- Load some tasks…
  IDL>  .r  /Applications/exelis/idl83/lib/reverse.pro


  IDL>  .r /Users/kwebb/IFU_reduction/idl_proc/plot_ppxf_30.pro
  IDL>  plot_ppxf_30

		INPUT: idl_proc/v2b_output_sn30_xbin_cent, idl_proc/v2b_output_sn30_ybin_cent, 
			idl_proc/ppxf/vel_output_sn30.txt, idl_proc/bin_spec_sn30/binned_cont_flux.lis
		OUTPUT: plots/continuum_velfield_sn30.png (saved manually)



CHECK IF VELOCITY SHIFT BETWEEN CCDS
————————————————————————————————————

In the plots we want to ensure the ‘rotation’ feature we can see (i.e. the velocity difference at the top of the galaxy and the bottom) is not a feature of a shift in the wavelength correction between the CCDs. 










####################################################################################################################
Test notes:

voronoi_2d_binning.py output at SN=30

	41  initial bins.
	Reassign bad bins...
	26  good bins.
	Modified Lloyd algorithm...
	Iter:    1  Diff: 139.2
	Iter:    2  Diff: 13.32
	Iter:    3  Diff: 6.005
	Iter:    4  Diff: 3.506
	Iter:    5  Diff: 1.943
	Iter:    6  Diff: 1.511
	Iter:    7  Diff: 0.9909
	Iter:    8  Diff: 0.4755
	Iter:    9  Diff: 0.5045
	Iter:   10  Diff: 0.2212
	Iter:   11  Diff: 0.1989
	Iter:   12  Diff: 0.06859
	Iter:   13  Diff: 0.1068
	Iter:   14  Diff: 0.1636
	Iter:   15  Diff: 0.08666
	Iter:   16  Diff: 0.07709
	Iter:   17  Diff: 0.1417
	Iter:   18  Diff: 0.06984
	Iter:   19  Diff: 0.0317
	Iter:   20  Diff: 0.009915
	Iter:   21  Diff: 0
	20  iterations.
	Unbinned pixels:  0  /  3600
	Fractional S/N scatter (%): 13.3416729619



#####################################################################################################################





—————
FXCOR
—————

ran fxcor with default parameters on binned spectra with SN 15 like gwen did (in her paper she mentions this)
output = fxcor_proc/fxcor_binned_spec_sn15*





——


